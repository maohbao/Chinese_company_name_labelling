{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version  3.8.6 | packaged by conda-forge | (default, Dec 26 2020, 05:05:16) \n",
      "[GCC 9.3.0]\n",
      "PyTorch version  1.7.1\n",
      "Current dir: /root/mao/249/bert-crf-company\n"
     ]
    }
   ],
   "source": [
    "# # # #\n",
    "#   - A pretrained BERT with CRF model.\n",
    "# # # #\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "from transformers import BertModel\n",
    "from transformers.modeling_bert import BertLayerNorm\n",
    "import pickle\n",
    "#from transformers import BertAdam, warmup_linear\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "def set_work_dir(local_path=\"bert-crf-company\", server_path=\"bert-crf-company\"):\n",
    "    if (os.path.exists(os.path.abspath('..')+'/'+local_path)):\n",
    "        os.chdir(os.path.abspath('..')+'/'+local_path)\n",
    "    elif (os.path.exists(os.path.abspath('..')+'/'+server_path)):\n",
    "        os.chdir(os.path.abspath('..')+'/'+server_path)\n",
    "    else:\n",
    "        raise Exception('Set work path error!')\n",
    "\n",
    "\n",
    "def get_data_dir(local_path=\"bert-crf-company/data\", server_path=\"bert-crf-company/data\"):\n",
    "    if (os.path.exists(os.path.abspath('..')+'/'+local_path)):\n",
    "        return os.path.abspath('..')+'/'+local_path\n",
    "    elif (os.path.exists(os.path.abspath('..')+'/'+server_path)):\n",
    "        return os.path.abspath('..')+'/'+server_path\n",
    "    else:\n",
    "        raise Exception('get data path error!')\n",
    "\n",
    "\n",
    "print('Python version ', sys.version)\n",
    "print('PyTorch version ', torch.__version__)\n",
    "\n",
    "set_work_dir()\n",
    "print('Current dir:', os.getcwd())\n",
    "\n",
    "data_dir = os.path.join(get_data_dir(), '')\n",
    "\n",
    "max_seq_length = 180 #256\n",
    "batch_size = 32 #32\n",
    "\n",
    "cuda_yes = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_yes else \"cpu\")\n",
    "\n",
    "#%%%%%%%%%%%%%%% if you want to use your own trained model, please change this dir %%%%%%%%%%%%\n",
    "output_dir = './output/'   ###    output\n",
    "\n",
    "if os.path.exists(output_dir+'/ner_bert_crf_checkpoint.pt')==False:\n",
    "    print('【No trained model!】')\n",
    "\n",
    "bert_model_scale = './bert-base-chinese'\n",
    "do_lower_case = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "'''\n",
    "Functions and Classes for read and organize data set\n",
    "'''\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for NER.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "          guid: Unique id for the example(a sentence or a pair of sentences).\n",
    "          words: list of words of sentence\n",
    "          labels_a/labels_b: (Optional) string. The label seqence of the text_a/text_b. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\n",
    "    result of convert_examples_to_features(InputExample)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids,  predict_mask, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.predict_mask = predict_mask\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @classmethod\n",
    "    def _read_data_demo(cls, input_file):\n",
    "        \"\"\"\n",
    "        Reads line\n",
    "        \"\"\"\n",
    "        with open(input_file) as f:\n",
    "            # out_lines = []\n",
    "            out_lists = []\n",
    "            entries = f.read().strip().splitlines()\n",
    "            \n",
    "            for line in entries:\n",
    "                words = []\n",
    "                ner_labels = []\n",
    "                for ch in line:\n",
    "                    words.append(ch)\n",
    "                    ner_labels.append('O')\n",
    "                out_lists.append([words,ner_labels])\n",
    "        return out_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyDataProcessor(DataProcessor):\n",
    "    def __init__(self):\n",
    "        self._label_types = [ 'X', '[CLS]', '[SEP]', 'O', 'I-LOC', 'B-LOC', 'I-NAME', 'I-INDU', 'I-ORG', 'B-NAME', 'B-INDU', 'B-ORG']\n",
    "        self._num_labels = len(self._label_types)\n",
    "        self._label_map = {label: i for i,\n",
    "                           label in enumerate(self._label_types)}\n",
    "\n",
    "    def get_demo_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_data_demo(os.path.join(data_dir, \"demo.txt\"))) ##########\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self._label_types\n",
    "\n",
    "    def get_num_labels(self):\n",
    "        return self.get_num_labels\n",
    "\n",
    "    def get_label_map(self):\n",
    "        return self._label_map\n",
    "    \n",
    "    def get_start_label_id(self):\n",
    "        return self._label_map['[CLS]']\n",
    "\n",
    "    def get_stop_label_id(self):\n",
    "        return self._label_map['[SEP]']\n",
    "\n",
    "    def _create_examples(self, all_lists):\n",
    "        examples = []\n",
    "        for (i, one_lists) in enumerate(all_lists):\n",
    "            guid = i\n",
    "            words = one_lists[0]\n",
    "            labels = one_lists[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, words=words, labels=labels))\n",
    "        return examples\n",
    "\n",
    "    def _create_examples2(self, lines):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = i\n",
    "            text = line[0]\n",
    "            ner_label = line[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, text_a=text, labels_a=ner_label))\n",
    "        return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(data.Dataset):\n",
    "    def __init__(self, examples, tokenizer, label_map, max_seq_length):\n",
    "        self.examples=examples\n",
    "        self.tokenizer=tokenizer\n",
    "        self.label_map=label_map\n",
    "        self.max_seq_length=max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat=example2feature(self.examples[idx], self.tokenizer, self.label_map, max_seq_length)\n",
    "        return feat.input_ids, feat.input_mask, feat.segment_ids, feat.predict_mask, feat.label_ids\n",
    "\n",
    "    @classmethod\n",
    "    def pad(cls, batch):\n",
    "\n",
    "        seqlen_list = [len(sample[0]) for sample in batch]\n",
    "        maxlen = np.array(seqlen_list).max()\n",
    "\n",
    "        f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: X for padding\n",
    "        input_ids_list = torch.LongTensor(f(0, maxlen))\n",
    "        input_mask_list = torch.LongTensor(f(1, maxlen))\n",
    "        segment_ids_list = torch.LongTensor(f(2, maxlen))\n",
    "        predict_mask_list = torch.ByteTensor(f(3, maxlen))\n",
    "        label_ids_list = torch.LongTensor(f(4, maxlen))\n",
    "\n",
    "        return input_ids_list, input_mask_list, segment_ids_list, predict_mask_list, label_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example2feature(example, tokenizer, label_map, max_seq_length):\n",
    "\n",
    "    add_label = 'X'\n",
    "    # tokenize_count = []\n",
    "    tokens = ['[CLS]']\n",
    "    predict_mask = [0]\n",
    "    label_ids = [label_map['[CLS]']]\n",
    "    for i, w in enumerate(example.words):\n",
    "        # use bertTokenizer to split words\n",
    "        sub_words = tokenizer.tokenize(w)\n",
    "        if not sub_words:\n",
    "            sub_words = ['[UNK]']\n",
    "        # tokenize_count.append(len(sub_words))\n",
    "        tokens.extend(sub_words)\n",
    "        for j in range(len(sub_words)):\n",
    "            if j == 0:\n",
    "                predict_mask.append(1)\n",
    "                label_ids.append(label_map[example.labels[i]])\n",
    "            else:\n",
    "                # '##xxx' -> 'X' (see bert paper)\n",
    "                predict_mask.append(0)\n",
    "                label_ids.append(label_map[add_label])\n",
    "\n",
    "    # truncate\n",
    "    if len(tokens) > max_seq_length - 1:\n",
    "        print('Example No.{} is too long, length is {}, truncated to {}!'.format(example.guid, len(tokens), max_seq_length))\n",
    "        tokens = tokens[0:(max_seq_length - 1)]\n",
    "        predict_mask = predict_mask[0:(max_seq_length - 1)]\n",
    "        label_ids = label_ids[0:(max_seq_length - 1)]\n",
    "    tokens.append('[SEP]')\n",
    "    predict_mask.append(0)\n",
    "    label_ids.append(label_map['[SEP]'])\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    segment_ids = [0] * len(input_ids)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    feat=InputFeatures(\n",
    "                # guid=example.guid,\n",
    "                # tokens=tokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                predict_mask=predict_mask,\n",
    "                label_ids=label_ids)\n",
    "\n",
    "    return feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Use BertModel + CRF ***\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "'''\n",
    "#####  Use BertModel + CRF  #####\n",
    "##### 直接用 bert，不用 BertForTokenClassification 了。\n",
    "CRF is for transition and the maximum likelyhood estimate(MLE).\n",
    "Bert is for latent label -> Emission of word embedding.\n",
    "'''\n",
    "print('*** Use BertModel + CRF ***')\n",
    "\n",
    "def log_sum_exp_1vec(vec):  # shape(1,m)\n",
    "    max_score = vec[0, np.argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def log_sum_exp_mat(log_M, axis=-1):  # shape(n,m)\n",
    "    return torch.max(log_M, axis)[0]+torch.log(torch.exp(log_M-torch.max(log_M, axis)[0][:, None]).sum(axis))\n",
    "\n",
    "def log_sum_exp_batch(log_Tensor, axis=-1): # shape (batch_size,n,m)\n",
    "    return torch.max(log_Tensor, axis)[0]+torch.log(torch.exp(log_Tensor-torch.max(log_Tensor, axis)[0].view(log_Tensor.shape[0],-1,1)).sum(axis))\n",
    "\n",
    "\n",
    "class BERT_CRF_NER(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model, start_label_id, stop_label_id, num_labels, max_seq_length, batch_size, device):\n",
    "        super(BERT_CRF_NER, self).__init__()\n",
    "        self.hidden_size = 768\n",
    "        self.start_label_id = start_label_id\n",
    "        self.stop_label_id = stop_label_id\n",
    "        self.num_labels = num_labels\n",
    "        # self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.device=device\n",
    "\n",
    "        # use pretrainded BertModel \n",
    "        self.bert = bert_model\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        # Maps the output of the bert into label space.\n",
    "        self.hidden2label = nn.Linear(self.hidden_size, self.num_labels)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.num_labels, self.num_labels))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer *to* the start tag(or label),\n",
    "        # and we never transfer *from* the stop label (the model would probably learn this anyway,\n",
    "        # so this enforcement is likely unimportant)\n",
    "        self.transitions.data[start_label_id, :] = -10000\n",
    "        self.transitions.data[:, stop_label_id] = -10000\n",
    "\n",
    "        nn.init.xavier_uniform_(self.hidden2label.weight)\n",
    "        nn.init.constant_(self.hidden2label.bias, 0.0)\n",
    "        # self.apply(self.init_bert_weights)\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)): \n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        '''\n",
    "        recursion: 递归\n",
    "        this also called alpha-recursion or forward recursion, to calculate log_prob of all barX \n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "        \n",
    "        # alpha_recursion,forward, alpha(zt)=p(zt,bar_x_1:t)\n",
    "        log_alpha = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
    "        # normal_alpha_0 : alpha[0]=Ot[0]*self.PIs\n",
    "        # self.start_label has all of the score. it is log,0 is p=1\n",
    "        log_alpha[:, 0, self.start_label_id] = 0\n",
    "        \n",
    "        # feats: sentances -> word embedding -> lstm -> MLP -> feats\n",
    "        # feats is the probability of emission, feat.shape=(1,tag_size)\n",
    "        for t in range(1, T):\n",
    "            log_alpha = (log_sum_exp_batch(self.transitions + log_alpha, axis=-1) + feats[:, t]).unsqueeze(1)\n",
    "\n",
    "        # log_prob of all barX\n",
    "        log_prob_all_barX = log_sum_exp_batch(log_alpha)\n",
    "        return log_prob_all_barX\n",
    "\n",
    "    def _get_bert_features(self, input_ids, segment_ids, input_mask):\n",
    "        '''\n",
    "        sentances -> word embedding -> lstm -> MLP -> feats\n",
    "        '''\n",
    "        bert_seq_out, _ = self.bert(input_ids, token_type_ids=segment_ids, attention_mask=input_mask, output_hidden_states=False)        \n",
    "        \n",
    "        bert_seq_out = self.dropout(bert_seq_out)\n",
    "        bert_feats = self.hidden2label(bert_seq_out)\n",
    "        return bert_feats\n",
    "\n",
    "    def _score_sentence(self, feats, label_ids):\n",
    "        ''' \n",
    "        Gives the score of a provided label sequence\n",
    "        p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        batch_transitions = self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
    "        batch_transitions = batch_transitions.flatten(1)\n",
    "\n",
    "        score = torch.zeros((feats.shape[0],1)).to(device)\n",
    "        # the 0th node is start_label->start_word,\bthe probability of them=1. so t begin with 1.\n",
    "        for t in range(1, T):\n",
    "            score = score + \\\n",
    "                batch_transitions.gather(-1, (label_ids[:, t]*self.num_labels+label_ids[:, t-1]).view(-1,1)) \\\n",
    "                    + feats[:, t].gather(-1, label_ids[:, t].view(-1,1)).view(-1,1)\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        '''\n",
    "        Max-Product Algorithm or viterbi algorithm, argmax(p(z_0:t|x_0:t))\n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        # batch_transitions=self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
    "\n",
    "        log_delta = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
    "        log_delta[:, 0, self.start_label_id] = 0\n",
    "        \n",
    "        # psi is for the vaule of the last latent that make P(this_latent) maximum.\n",
    "        psi = torch.zeros((batch_size, T, self.num_labels), dtype=torch.long).to(self.device)  # psi[0]=0000 useless\n",
    "        for t in range(1, T):\n",
    "            # delta[t][k]=max_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # delta[t] is the max prob of the path from  z_t-1 to z_t[k]\n",
    "            log_delta, psi[:, t] = torch.max(self.transitions + log_delta, -1)\n",
    "            # psi[t][k]=argmax_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # psi[t][k] is the path choosed from z_t-1 to z_t[k],the value is the z_state(is k) index of z_t-1\n",
    "            log_delta = (log_delta + feats[:, t]).unsqueeze(1)\n",
    "\n",
    "        # trace back\n",
    "        path = torch.zeros((batch_size, T), dtype=torch.long).to(self.device)\n",
    "\n",
    "        # max p(z1:t,all_x|theta)\n",
    "        max_logLL_allz_allx, path[:, -1] = torch.max(log_delta.squeeze(), -1)\n",
    "\n",
    "        for t in range(T-2, -1, -1):\n",
    "            # choose the state of z_t according the state choosed of z_t+1.\n",
    "            path[:, t] = psi[:, t+1].gather(-1,path[:, t+1].view(-1,1)).squeeze()\n",
    "\n",
    "        return max_logLL_allz_allx, path\n",
    "\n",
    "    ### 注意，train 的时候调用的是这个函数；这个函数里再调用 _forward_alg\n",
    "    def neg_log_likelihood(self, input_ids, segment_ids, input_mask, label_ids):\n",
    "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
    "        forward_score = self._forward_alg(bert_feats)\n",
    "        # p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
    "        gold_score = self._score_sentence(bert_feats, label_ids)\n",
    "        # - log[ p(X=w1:t,Zt=tag1:t)/p(X=w1:t) ] = - log[ p(Zt=tag1:t|X=w1:t) ]\n",
    "        return torch.mean(forward_score - gold_score)\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, input_mask):\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, label_seq_ids = self._viterbi_decode(bert_feats)\n",
    "        return score, label_seq_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the pretrain NER_BERT_CRF model, epoch: 10 valid acc: 0.9967663702506063 valid f1: 0.9932473910374464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERT_CRF_NER(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (hidden2label): Linear(in_features=768, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "# load NER model \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_scale, do_lower_case=do_lower_case)\n",
    "\n",
    "companyProcessor = CompanyDataProcessor()\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "label_list = companyProcessor.get_labels()\n",
    "label_map = companyProcessor.get_label_map()\n",
    "\n",
    "demo_examples = companyProcessor.get_demo_examples(data_dir)\n",
    "demo_dataset = NerDataset(demo_examples,tokenizer,label_map,max_seq_length)\n",
    "demo_dataloader = data.DataLoader(dataset=demo_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "\n",
    "start_label_id = companyProcessor.get_start_label_id()\n",
    "stop_label_id = companyProcessor.get_stop_label_id()\n",
    "\n",
    "bert_model = BertModel.from_pretrained(bert_model_scale)\n",
    "\n",
    "model = BERT_CRF_NER(bert_model, start_label_id, stop_label_id, len(label_list), max_seq_length, batch_size, device)\n",
    "\n",
    "checkpoint = torch.load(output_dir+'/ner_bert_crf_checkpoint.pt', map_location='cpu')\n",
    "start_epoch = checkpoint['epoch']+1\n",
    "valid_acc_prev = checkpoint['valid_acc']\n",
    "valid_f1_prev = checkpoint['valid_f1']\n",
    "pretrained_dict=checkpoint['model_state']\n",
    "net_state_dict = model.state_dict()\n",
    "pretrained_dict_selected = {k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "net_state_dict.update(pretrained_dict_selected)\n",
    "model.load_state_dict(net_state_dict)\n",
    "print('Loaded the pretrain NER_BERT_CRF model, epoch:',checkpoint['epoch'],'valid acc:', \n",
    "        checkpoint['valid_acc'], 'valid f1:', checkpoint['valid_f1'])\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[武汉]【翔海】{水处理}<有限公司>\n",
      "[漯河市]【天利】{白瓜籽加工}<有限责任公司>\n",
      "[大连]【泰和道】{农业发展}<有限公司>\n",
      "[天津]【大致】【和堂】{医药}<有限公司>【延年】{中药}<店>\n",
      "[上海]【佳吉】{快运}<有限公司>[余姚市]【低塘】<营业部>\n",
      "[上饶市]【百业】【众赢】{网络科技}<有限公司>\n",
      "[本溪市]【枫汤居】{宾馆}<有限公司>\n",
      "[四川省]【鸿利达】{涂装科技}<有限责任公司>\n",
      "[武汉]【爱尔】{眼科医院}\n",
      "[通海县]{电影院}\n",
      "【邹易儒】([北京]){影视文化}<工作室>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-d950e42c32b4>:18: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "  valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "i_batch=0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    demon_dataloader = data.DataLoader(dataset=demo_dataset,\n",
    "                                batch_size=10, #######\n",
    "                                shuffle=False,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "    for batch in demon_dataloader:\n",
    "        i_batch+=1\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "        _, predicted_label_seq_ids = model(input_ids, segment_ids, input_mask)\n",
    "        # _, predicted = torch.max(out_scores, -1)\n",
    "        valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)\n",
    "        # valid_label_ids = torch.masked_select(label_ids, predict_mask)\n",
    "        for i in range(len(input_ids)):  #### 10\n",
    "\n",
    "            new_ids=predicted_label_seq_ids[i].cpu().numpy()[predict_mask[i].cpu().numpy()==1]\n",
    "\n",
    "            predicted_label=list(map(lambda i: label_list[i], new_ids))\n",
    "            #print(predicted_label)\n",
    "\n",
    "            ls_words=demo_examples[(i_batch-1)*10+i].words\n",
    "            str_words=''\n",
    "            bStart_LOC=False\n",
    "            bStart_NAME=False            \n",
    "            bStart_INDU=False\n",
    "            bStart_ORG=False\n",
    "            str_words=''\n",
    "            for word, label in zip(ls_words, predicted_label):\n",
    "                ### 起始符号\n",
    "                if label.find('B-')>=0 and len(str_words)==0:\n",
    "                    if label.find('LOC')>0:\n",
    "                        str_words+='['\n",
    "                        bStart_LOC=True                       \n",
    "                    elif label.find('NAME')>0:\n",
    "                        str_words+='【'\n",
    "                        bStart_NAME=True                                                \n",
    "                    elif label.find('INDU')>0:\n",
    "                        str_words+='{'    \n",
    "                        bStart_INDU=True\n",
    "                    elif label.find('ORG')>0:\n",
    "                        str_words+='<'       \n",
    "                        bStart_ORG=True   \n",
    "                ### 中间符号\n",
    "                elif label.find('B-')>=0:\n",
    "                    str_end=''\n",
    "                    str_start=''\n",
    "                    #结束上一个状态\n",
    "                    if bStart_LOC:\n",
    "                        bStart_LOC=False\n",
    "                        str_end=']'\n",
    "                    if bStart_NAME:\n",
    "                        bStart_NAME=False\n",
    "                        str_end='】'\n",
    "                    if bStart_INDU:\n",
    "                        bStart_INDU=False\n",
    "                        str_end='}'\n",
    "                    if bStart_ORG:\n",
    "                        bStart_ORG=False\n",
    "                        str_end='>'  \n",
    "                     \n",
    "                    #开始新的状态\n",
    "                    if label.find('LOC')>0:\n",
    "                        bStart_LOC=True\n",
    "                        str_start='['\n",
    "                    if label.find('NAME')>0:\n",
    "                        bStart_NAME=True\n",
    "                        str_start='【'\n",
    "                    if label.find('INDU')>0:\n",
    "                        bStart_INDU=True\n",
    "                        str_start='{'\n",
    "                    if label.find('ORG')>0:\n",
    "                        bStart_ORG=True\n",
    "                        str_start='<'    \n",
    "                    \n",
    "                    str_words+=str_end+str_start\n",
    "                    \n",
    "                elif label=='O':\n",
    "                    str_end=''\n",
    "                    #结束上一个状态\n",
    "                    if bStart_LOC:\n",
    "                        bStart_LOC=False\n",
    "                        str_end=']'\n",
    "                    if bStart_NAME:\n",
    "                        bStart_NAME=False\n",
    "                        str_end='】'\n",
    "                    if bStart_INDU:\n",
    "                        bStart_INDU=False\n",
    "                        str_end='}'\n",
    "                    if bStart_ORG:\n",
    "                        bStart_ORG=False\n",
    "                        str_end='>'       \n",
    "                        \n",
    "                    str_words+=str_end\n",
    "                        \n",
    "                str_words+=word\n",
    "                \n",
    "            ### 结尾符号    \n",
    "            if bStart_LOC:\n",
    "                str_words+=']'\n",
    "            if bStart_NAME:\n",
    "                str_words+='】'\n",
    "            if bStart_INDU:\n",
    "                str_words+='}'\n",
    "            if bStart_ORG:\n",
    "                str_words+='>'  \n",
    "                \n",
    "            print(str_words)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wiser)",
   "language": "python",
   "name": "wiser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
