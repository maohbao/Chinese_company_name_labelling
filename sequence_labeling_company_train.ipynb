{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version  3.8.6 | packaged by conda-forge | (default, Dec 26 2020, 05:05:16) \n",
      "[GCC 9.3.0]\n",
      "PyTorch version  1.7.1\n",
      "Current dir: /root/mao/249/bert-crf-company\n",
      "Cuda is available? True\n",
      "Device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# # # #\n",
    "#   - A pretrained BERT with CRF model.\n",
    "# # # #\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils import data\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import collections\n",
    "\n",
    "from transformers import BertModel, BertForTokenClassification\n",
    "from transformers.modeling_bert import BertLayerNorm\n",
    "import pickle\n",
    "#from transformers import BertAdam, warmup_linear\n",
    "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer\n",
    "from seqeval.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def set_work_dir(local_path=\"bert-crf-company\", server_path=\"bert-crf-company\"):\n",
    "    if (os.path.exists(os.path.abspath('..')+'/'+local_path)):\n",
    "        os.chdir(os.path.abspath('..')+'/'+local_path)\n",
    "    elif (os.path.exists(os.path.abspath('..')+'/'+server_path)):\n",
    "        os.chdir(os.path.abspath('..')+'/'+server_path)\n",
    "    else:\n",
    "        raise Exception('Set work path error!')\n",
    "\n",
    "\n",
    "def get_data_dir(local_path=\"bert-crf-company/data\", server_path=\"bert-crf-company/data\"):\n",
    "    if (os.path.exists(os.path.abspath('..')+'/'+local_path)):\n",
    "        return os.path.abspath('..')+'/'+local_path\n",
    "    elif (os.path.exists(os.path.abspath('..')+'/'+server_path)):\n",
    "        return os.path.abspath('..')+'/'+server_path\n",
    "    else:\n",
    "        raise Exception('get data path error!')\n",
    "\n",
    "\n",
    "print('Python version ', sys.version)\n",
    "print('PyTorch version ', torch.__version__)\n",
    "\n",
    "set_work_dir()\n",
    "print('Current dir:', os.getcwd())\n",
    "\n",
    "### 是否使用 GPU\n",
    "cuda_yes = torch.cuda.is_available()\n",
    "# cuda_yes = True\n",
    "\n",
    "print('Cuda is available?', cuda_yes)\n",
    "device = torch.device(\"cuda:1\" if cuda_yes else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "data_dir = os.path.join(get_data_dir(), '')\n",
    "# \"Whether to run training.\"\n",
    "do_train = True\n",
    "# \"Whether to run eval on the dev set.\"\n",
    "do_eval = True\n",
    "# \"Whether to run the model in inference mode on the test set.\"\n",
    "do_predict = True\n",
    "# Whether load checkpoint file before train model\n",
    "load_checkpoint = True\n",
    "# \"The vocabulary file that the BERT model was trained on.\"\n",
    "max_seq_length = 180 #256\n",
    "batch_size = 32 #32\n",
    "# \"The initial learning rate for Adam.\"\n",
    "learning_rate0 = 5e-5\n",
    "lr0_crf_fc = 8e-5\n",
    "weight_decay_finetune = 1e-5 #0.01\n",
    "weight_decay_crf_fc = 5e-6 #0.005\n",
    "total_train_epochs = 15\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "warmup_proportion = 0.1\n",
    "\n",
    "### 模型输出目录 ###\n",
    "output_dir = './output/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "bert_model_scale = './bert-base-chinese'\n",
    "do_lower_case = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "'''\n",
    "Functions and Classes for read and organize data set\n",
    "'''\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for NER.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "          guid: Unique id for the example(a sentence or a pair of sentences).\n",
    "          words: list of words of sentence\n",
    "          labels_a/labels_b: (Optional) string. The label seqence of the text_a/text_b. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\n",
    "    result of convert_examples_to_features(InputExample)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids,  predict_mask, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.predict_mask = predict_mask\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"\n",
    "        Reads a BIO data.\n",
    "        \"\"\"\n",
    "        with open(input_file) as f:\n",
    "            # out_lines = []\n",
    "            out_lists = []\n",
    "            entries = f.read().strip().split(\"\\n\")\n",
    "            for line in entries:\n",
    "                entries_ = line.split(' ')\n",
    "                words = []\n",
    "                ner_labels = []                \n",
    "                for entry in entries_:\n",
    "                    ls_entry=entry.split('/')\n",
    "                    assert len(ls_entry)==2\n",
    "                    words.append(ls_entry[0])\n",
    "                    ner_labels.append(ls_entry[1])\n",
    "\n",
    "                out_lists.append([words,ner_labels])\n",
    "        return out_lists\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def _read_data_demo(cls, input_file):\n",
    "        \"\"\"\n",
    "        Reads line\n",
    "        \"\"\"\n",
    "        with open(input_file) as f:\n",
    "            # out_lines = []\n",
    "            out_lists = []\n",
    "            entries = f.read().strip().splitlines()\n",
    "            \n",
    "            for line in entries:\n",
    "                words = []\n",
    "                ner_labels = []\n",
    "                for ch in line:\n",
    "                    words.append(ch)\n",
    "                    ner_labels.append('O')\n",
    "                out_lists.append([words,ner_labels])\n",
    "        return out_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyDataProcessor(DataProcessor):\n",
    "    def __init__(self):\n",
    "        self._label_types = [ 'X', '[CLS]', '[SEP]', 'O', 'I-LOC', 'B-LOC', 'I-NAME', 'I-INDU', 'I-ORG', 'B-NAME', 'B-INDU', 'B-ORG']\n",
    "        self._num_labels = len(self._label_types)\n",
    "        self._label_map = {label: i for i,\n",
    "                           label in enumerate(self._label_types)}\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_data(os.path.join(data_dir, \"train.1000.txt\")))\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_data(os.path.join(data_dir, \"dev.200.txt\")))\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_data(os.path.join(data_dir, \"test.200.txt\"))) \n",
    "\n",
    "    def get_labels(self):\n",
    "        return self._label_types\n",
    "\n",
    "    def get_num_labels(self):\n",
    "        return self.get_num_labels\n",
    "\n",
    "    def get_label_map(self):\n",
    "        return self._label_map\n",
    "    \n",
    "    def get_start_label_id(self):\n",
    "        return self._label_map['[CLS]']\n",
    "\n",
    "    def get_stop_label_id(self):\n",
    "        return self._label_map['[SEP]']\n",
    "\n",
    "    def _create_examples(self, all_lists):\n",
    "        examples = []\n",
    "        for (i, one_lists) in enumerate(all_lists):\n",
    "            guid = i\n",
    "            words = one_lists[0]\n",
    "            labels = one_lists[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, words=words, labels=labels))\n",
    "        return examples\n",
    "\n",
    "    def _create_examples2(self, lines):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = i\n",
    "            text = line[0]\n",
    "            ner_label = line[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, text_a=text, labels_a=ner_label))\n",
    "        return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(data.Dataset):\n",
    "    def __init__(self, examples, tokenizer, label_map, max_seq_length):\n",
    "        self.examples=examples\n",
    "        self.tokenizer=tokenizer\n",
    "        self.label_map=label_map\n",
    "        self.max_seq_length=max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat=example2feature(self.examples[idx], self.tokenizer, self.label_map, max_seq_length)\n",
    "        return feat.input_ids, feat.input_mask, feat.segment_ids, feat.predict_mask, feat.label_ids\n",
    "\n",
    "    @classmethod\n",
    "    def pad(cls, batch):\n",
    "\n",
    "        seqlen_list = [len(sample[0]) for sample in batch]\n",
    "        maxlen = np.array(seqlen_list).max()\n",
    "\n",
    "        f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: X for padding\n",
    "        input_ids_list = torch.LongTensor(f(0, maxlen))\n",
    "        input_mask_list = torch.LongTensor(f(1, maxlen))\n",
    "        segment_ids_list = torch.LongTensor(f(2, maxlen))\n",
    "        predict_mask_list = torch.ByteTensor(f(3, maxlen))\n",
    "        label_ids_list = torch.LongTensor(f(4, maxlen))\n",
    "\n",
    "        return input_ids_list, input_mask_list, segment_ids_list, predict_mask_list, label_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "  Num steps = 468\n"
     ]
    }
   ],
   "source": [
    "def example2feature(example, tokenizer, label_map, max_seq_length):\n",
    "\n",
    "    add_label = 'X'\n",
    "    # tokenize_count = []\n",
    "    tokens = ['[CLS]']\n",
    "    predict_mask = [0]\n",
    "    label_ids = [label_map['[CLS]']]\n",
    "    for i, w in enumerate(example.words):\n",
    "        # use bertTokenizer to split words\n",
    "        sub_words = tokenizer.tokenize(w)\n",
    "        if not sub_words:\n",
    "            sub_words = ['[UNK]']\n",
    "        # tokenize_count.append(len(sub_words))\n",
    "        tokens.extend(sub_words)\n",
    "        for j in range(len(sub_words)):\n",
    "            if j == 0:\n",
    "                predict_mask.append(1)\n",
    "                label_ids.append(label_map[example.labels[i]])\n",
    "            else:\n",
    "                # '##xxx' -> 'X' (see bert paper)\n",
    "                predict_mask.append(0)\n",
    "                label_ids.append(label_map[add_label])\n",
    "\n",
    "    # truncate\n",
    "    if len(tokens) > max_seq_length - 1:\n",
    "        print('Example No.{} is too long, length is {}, truncated to {}!'.format(example.guid, len(tokens), max_seq_length))\n",
    "        tokens = tokens[0:(max_seq_length - 1)]\n",
    "        predict_mask = predict_mask[0:(max_seq_length - 1)]\n",
    "        label_ids = label_ids[0:(max_seq_length - 1)]\n",
    "    tokens.append('[SEP]')\n",
    "    predict_mask.append(0)\n",
    "    label_ids.append(label_map['[SEP]'])\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    segment_ids = [0] * len(input_ids)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    feat=InputFeatures(\n",
    "                # guid=example.guid,\n",
    "                # tokens=tokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                predict_mask=predict_mask,\n",
    "                label_ids=label_ids)\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n",
    "#%%\n",
    "'''\n",
    "Prepare data set\n",
    "'''\n",
    "# random.seed(44)\n",
    "np.random.seed(44)\n",
    "torch.manual_seed(44)\n",
    "if cuda_yes:\n",
    "    torch.cuda.manual_seed_all(44)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "companyProcessor = CompanyDataProcessor()\n",
    "label_list = companyProcessor.get_labels()\n",
    "label_map = companyProcessor.get_label_map()\n",
    "train_examples = companyProcessor.get_train_examples(data_dir)\n",
    "dev_examples = companyProcessor.get_dev_examples(data_dir)\n",
    "test_examples = companyProcessor.get_test_examples(data_dir)\n",
    "\n",
    "total_train_steps = int(len(train_examples) / batch_size / gradient_accumulation_steps * total_train_epochs)\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"% len(train_examples))\n",
    "print(\"  Batch size = %d\"% batch_size)\n",
    "print(\"  Num steps = %d\"% total_train_steps)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_scale, do_lower_case=do_lower_case)\n",
    "\n",
    "train_dataset = NerDataset(train_examples,tokenizer,label_map,max_seq_length)\n",
    "dev_dataset = NerDataset(dev_examples,tokenizer,label_map,max_seq_length)\n",
    "test_dataset = NerDataset(test_examples,tokenizer,label_map,max_seq_length)\n",
    "\n",
    "train_dataloader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "\n",
    "dev_dataloader = data.DataLoader(dataset=dev_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "\n",
    "test_dataloader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Use BertModel + CRF ***\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "'''\n",
    "#####  Use BertModel + CRF  #####\n",
    "##### 直接用 bert，不用 BertForTokenClassification 了。\n",
    "CRF is for transition and the maximum likelyhood estimate(MLE).\n",
    "Bert is for latent label -> Emission of word embedding.\n",
    "'''\n",
    "\n",
    "print('*** Use BertModel + CRF ***')\n",
    "\n",
    "\n",
    "def log_sum_exp_1vec(vec):  # shape(1,m)\n",
    "    max_score = vec[0, np.argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def log_sum_exp_mat(log_M, axis=-1):  # shape(n,m)\n",
    "    return torch.max(log_M, axis)[0]+torch.log(torch.exp(log_M-torch.max(log_M, axis)[0][:, None]).sum(axis))\n",
    "\n",
    "def log_sum_exp_batch(log_Tensor, axis=-1): # shape (batch_size,n,m)\n",
    "    return torch.max(log_Tensor, axis)[0]+torch.log(torch.exp(log_Tensor-torch.max(log_Tensor, axis)[0].view(log_Tensor.shape[0],-1,1)).sum(axis))\n",
    "\n",
    "\n",
    "class BERT_CRF_NER(nn.Module):\n",
    "    def __init__(self, bert_model, start_label_id, stop_label_id, num_labels, max_seq_length, batch_size, device):\n",
    "        super(BERT_CRF_NER, self).__init__()\n",
    "        self.hidden_size = 768\n",
    "        self.start_label_id = start_label_id\n",
    "        self.stop_label_id = stop_label_id\n",
    "        self.num_labels = num_labels\n",
    "        # self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.device=device\n",
    "\n",
    "        # use pretrainded BertModel \n",
    "        self.bert = bert_model\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        # Maps the output of the bert into label space.\n",
    "        self.hidden2label = nn.Linear(self.hidden_size, self.num_labels)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.num_labels, self.num_labels))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer *to* the start tag(or label),\n",
    "        # and we never transfer *from* the stop label (the model would probably learn this anyway,\n",
    "        # so this enforcement is likely unimportant)\n",
    "        self.transitions.data[start_label_id, :] = -10000\n",
    "        self.transitions.data[:, stop_label_id] = -10000\n",
    "\n",
    "        nn.init.xavier_uniform_(self.hidden2label.weight)\n",
    "        nn.init.constant_(self.hidden2label.bias, 0.0)\n",
    "        # self.apply(self.init_bert_weights)\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)): \n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "    \n",
    "    def _forward_alg(self, feats):\n",
    "        '''\n",
    "        recursion: 递归\n",
    "        this also called alpha-recursion or forward recursion, to calculate log_prob of all barX \n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "        \n",
    "        # alpha_recursion,forward, alpha(zt)=p(zt,bar_x_1:t)\n",
    "        log_alpha = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
    "        # normal_alpha_0 : alpha[0]=Ot[0]*self.PIs\n",
    "        # self.start_label has all of the score. it is log,0 is p=1\n",
    "        log_alpha[:, 0, self.start_label_id] = 0\n",
    "        \n",
    "        # feats: sentances -> word embedding -> lstm -> MLP -> feats\n",
    "        # feats is the probability of emission, feat.shape=(1,tag_size)\n",
    "        for t in range(1, T):\n",
    "            log_alpha = (log_sum_exp_batch(self.transitions + log_alpha, axis=-1) + feats[:, t]).unsqueeze(1)\n",
    "\n",
    "        # log_prob of all barX\n",
    "        log_prob_all_barX = log_sum_exp_batch(log_alpha)\n",
    "        return log_prob_all_barX\n",
    "\n",
    "    def _get_bert_features(self, input_ids, segment_ids, input_mask):\n",
    "        '''\n",
    "        sentances -> word embedding -> lstm -> MLP -> feats\n",
    "        '''\n",
    "        bert_seq_out, _ = self.bert(input_ids, token_type_ids=segment_ids, attention_mask=input_mask, output_hidden_states=False)        \n",
    "        \n",
    "        bert_seq_out = self.dropout(bert_seq_out)\n",
    "        bert_feats = self.hidden2label(bert_seq_out)\n",
    "        return bert_feats\n",
    "\n",
    "    def _score_sentence(self, feats, label_ids):\n",
    "        ''' \n",
    "        Gives the score of a provided label sequence\n",
    "        p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        batch_transitions = self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
    "        batch_transitions = batch_transitions.flatten(1)\n",
    "\n",
    "        score = torch.zeros((feats.shape[0],1)).to(device)\n",
    "        # the 0th node is start_label->start_word,\bthe probability of them=1. so t begin with 1.\n",
    "        for t in range(1, T):\n",
    "            score = score + \\\n",
    "                batch_transitions.gather(-1, (label_ids[:, t]*self.num_labels+label_ids[:, t-1]).view(-1,1)) \\\n",
    "                    + feats[:, t].gather(-1, label_ids[:, t].view(-1,1)).view(-1,1)\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        '''\n",
    "        Max-Product Algorithm or viterbi algorithm, argmax(p(z_0:t|x_0:t))\n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        # batch_transitions=self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
    "\n",
    "        log_delta = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
    "        log_delta[:, 0, self.start_label_id] = 0\n",
    "        \n",
    "        # psi is for the vaule of the last latent that make P(this_latent) maximum.\n",
    "        psi = torch.zeros((batch_size, T, self.num_labels), dtype=torch.long).to(self.device)  # psi[0]=0000 useless\n",
    "        for t in range(1, T):\n",
    "            # delta[t][k]=max_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # delta[t] is the max prob of the path from  z_t-1 to z_t[k]\n",
    "            log_delta, psi[:, t] = torch.max(self.transitions + log_delta, -1)\n",
    "            # psi[t][k]=argmax_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # psi[t][k] is the path choosed from z_t-1 to z_t[k],the value is the z_state(is k) index of z_t-1\n",
    "            log_delta = (log_delta + feats[:, t]).unsqueeze(1)\n",
    "\n",
    "        # trace back\n",
    "        path = torch.zeros((batch_size, T), dtype=torch.long).to(self.device)\n",
    "\n",
    "        # max p(z1:t,all_x|theta)\n",
    "        max_logLL_allz_allx, path[:, -1] = torch.max(log_delta.squeeze(), -1)\n",
    "\n",
    "        for t in range(T-2, -1, -1):\n",
    "            # choose the state of z_t according the state choosed of z_t+1.\n",
    "            path[:, t] = psi[:, t+1].gather(-1,path[:, t+1].view(-1,1)).squeeze()\n",
    "\n",
    "        return max_logLL_allz_allx, path\n",
    "\n",
    "\n",
    "    def neg_log_likelihood(self, input_ids, segment_ids, input_mask, label_ids):\n",
    "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
    "        forward_score = self._forward_alg(bert_feats)\n",
    "        # p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
    "        gold_score = self._score_sentence(bert_feats, label_ids)\n",
    "        # - log[ p(X=w1:t,Zt=tag1:t)/p(X=w1:t) ] = - log[ p(Zt=tag1:t|X=w1:t) ]\n",
    "        return torch.mean(forward_score - gold_score)\n",
    "\n",
    "    # this forward is just for predict, not for train\n",
    "    # dont confuse this with _forward_alg above.\n",
    "    def forward(self, input_ids, segment_ids, input_mask):\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, label_seq_ids = self._viterbi_decode(bert_feats)\n",
    "        return score, label_seq_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "# defind NER model \n",
    "\n",
    "start_label_id = companyProcessor.get_start_label_id()\n",
    "stop_label_id = companyProcessor.get_stop_label_id()\n",
    "\n",
    "bert_model = BertModel.from_pretrained(bert_model_scale)\n",
    "# from transformers import BertConfig \n",
    "# model_config = BertConfig.from_pretrained(bert_model_scale, output_hidden_states=True)\n",
    "# bert_model = BertModel.from_pretrained(bert_model_scale, config=model_config)\n",
    "\n",
    "\n",
    "model = BERT_CRF_NER(bert_model, start_label_id, stop_label_id, len(label_list), max_seq_length, batch_size, device)\n",
    "\n",
    "\n",
    "if load_checkpoint and os.path.exists(output_dir+'/ner_bert_crf_checkpoint.pt'):\n",
    "    checkpoint = torch.load(output_dir+'/ner_bert_crf_checkpoint.pt', map_location='cpu')\n",
    "    start_epoch = checkpoint['epoch']+1\n",
    "    valid_acc_prev = checkpoint['valid_acc']\n",
    "    valid_f1_prev = checkpoint['valid_f1']\n",
    "    pretrained_dict=checkpoint['model_state']\n",
    "    net_state_dict = model.state_dict()\n",
    "    pretrained_dict_selected = {k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "    net_state_dict.update(pretrained_dict_selected)\n",
    "    model.load_state_dict(net_state_dict)\n",
    "    print('Loaded the pretrain NER_BERT_CRF model, epoch:',checkpoint['epoch'],'valid acc:', \n",
    "            checkpoint['valid_acc'], 'valid f1:', checkpoint['valid_f1'])\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    valid_acc_prev = 0\n",
    "    valid_f1_prev = 0\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "new_param = ['transitions', 'hidden2label.weight', 'hidden2label.bias']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) \\\n",
    "        and not any(nd in n for nd in new_param)], 'weight_decay': weight_decay_finetune},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) \\\n",
    "        and not any(nd in n for nd in new_param)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in param_optimizer if n in ('transitions','hidden2label.weight')] \\\n",
    "        , 'lr':lr0_crf_fc, 'weight_decay': weight_decay_crf_fc},\n",
    "    {'params': [p for n, p in param_optimizer if n == 'hidden2label.bias'] \\\n",
    "        , 'lr':lr0_crf_fc, 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# optimizer = BertAdam(optimizer_grouped_parameters, lr=learning_rate0, warmup=warmup_proportion, t_total=total_train_steps)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate0)\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, predict_dataloader, batch_size, epoch_th, dataset_name):\n",
    "    # print(\"***** Running prediction *****\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total=0\n",
    "    correct=0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in predict_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "            \n",
    "            _, predicted_label_seq_ids = model(input_ids, segment_ids, input_mask)\n",
    "            \n",
    "            \n",
    "            # _, predicted = torch.max(out_scores, -1)\n",
    "            valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)\n",
    "            valid_label_ids = torch.masked_select(label_ids, predict_mask)\n",
    "            \n",
    "            all_preds.extend(valid_predicted.tolist())\n",
    "            all_labels.extend(valid_label_ids.tolist())\n",
    "            # print(len(valid_label_ids),len(valid_predicted),len(valid_label_ids)==len(valid_predicted))\n",
    "            total += len(valid_label_ids)\n",
    "            correct += valid_predicted.eq(valid_label_ids).sum().item()\n",
    "\n",
    "    all_labels_tag=[label_list[i] for i in all_labels]\n",
    "    all_preds_tag=[label_list[i] for i in all_preds]\n",
    "\n",
    "    test_acc = correct/total\n",
    "\n",
    "    f1=f1_score([all_labels_tag], [all_preds_tag])\n",
    "    print(\"F1-Score: {}\".format(f1))\n",
    "    print(\"Classification report: -- \")\n",
    "    ### seqeval.metrics 中的函数：classification_report\n",
    "    print(classification_report([all_labels_tag], [all_preds_tag]))\n",
    "    \n",
    "    end = time.time()\n",
    "    print('Epoch:%d, Acc:%.2f, on %s, Spend:%.3f minutes for evaluation' \\\n",
    "        % (epoch_th, 100.*test_acc, dataset_name,(end-start)/60.0))\n",
    "    print('--------------------------------------------------------------')\n",
    "    return test_acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0-0/32, Negative loglikelihood: 9743.21484375 \n",
      "Epoch:0-1/32, Negative loglikelihood: 9765.427734375 \n",
      "Epoch:0-2/32, Negative loglikelihood: 9763.287109375 \n",
      "Epoch:0-3/32, Negative loglikelihood: 9740.1259765625 \n",
      "Epoch:0-4/32, Negative loglikelihood: 9742.478515625 \n",
      "Epoch:0-5/32, Negative loglikelihood: 9425.0498046875 \n",
      "Epoch:0-6/32, Negative loglikelihood: 9730.8359375 \n",
      "Epoch:0-7/32, Negative loglikelihood: 9736.9765625 \n",
      "Epoch:0-8/32, Negative loglikelihood: 9731.392578125 \n",
      "Epoch:0-9/32, Negative loglikelihood: 9721.7763671875 \n",
      "Epoch:0-10/32, Negative loglikelihood: 9719.5595703125 \n",
      "Epoch:0-11/32, Negative loglikelihood: 9408.3671875 \n",
      "Epoch:0-12/32, Negative loglikelihood: 9714.478515625 \n",
      "Epoch:0-13/32, Negative loglikelihood: 9712.365234375 \n",
      "Epoch:0-14/32, Negative loglikelihood: 9708.087890625 \n",
      "Epoch:0-15/32, Negative loglikelihood: 9705.453125 \n",
      "Epoch:0-16/32, Negative loglikelihood: 9703.2001953125 \n",
      "Epoch:0-17/32, Negative loglikelihood: 9699.1552734375 \n",
      "Epoch:0-18/32, Negative loglikelihood: 9697.76171875 \n",
      "Epoch:0-19/32, Negative loglikelihood: 9693.529296875 \n",
      "Epoch:0-20/32, Negative loglikelihood: 9379.1787109375 \n",
      "Epoch:0-21/32, Negative loglikelihood: 9689.291015625 \n",
      "Epoch:0-22/32, Negative loglikelihood: 9689.21484375 \n",
      "Epoch:0-23/32, Negative loglikelihood: 9372.54296875 \n",
      "Epoch:0-24/32, Negative loglikelihood: 9371.482421875 \n",
      "Epoch:0-25/32, Negative loglikelihood: 9680.48828125 \n",
      "Epoch:0-26/32, Negative loglikelihood: 9678.888671875 \n",
      "Epoch:0-27/32, Negative loglikelihood: 9679.193359375 \n",
      "Epoch:0-28/32, Negative loglikelihood: 9675.9453125 \n",
      "Epoch:0-29/32, Negative loglikelihood: 9674.759765625 \n",
      "Epoch:0-30/32, Negative loglikelihood: 9361.685546875 \n",
      "Epoch:0-31/32, Negative loglikelihood: 8737.7529296875 \n",
      "--------------------------------------------------------------\n",
      "Epoch:0 completed, Total training's Loss: 307852.947265625, Spend: 0.07141781250635783m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-2ab4ba9539d3>:19: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "  valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)\n",
      "<ipython-input-8-2ab4ba9539d3>:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "  valid_label_ids = torch.masked_select(label_ids, predict_mask)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9469240048250904\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.91      0.98      0.94       200\n",
      "         LOC       0.96      0.98      0.97       203\n",
      "        NAME       0.93      0.95      0.94       208\n",
      "         ORG       0.92      0.96      0.94       203\n",
      "\n",
      "   micro avg       0.93      0.96      0.95       814\n",
      "   macro avg       0.93      0.96      0.95       814\n",
      "weighted avg       0.93      0.96      0.95       814\n",
      "\n",
      "Epoch:0, Acc:98.02, on Valid_set, Spend:0.007 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:1-0/32, Negative loglikelihood: 9673.21484375 \n",
      "Epoch:1-1/32, Negative loglikelihood: 9671.703125 \n",
      "Epoch:1-2/32, Negative loglikelihood: 9669.615234375 \n",
      "Epoch:1-3/32, Negative loglikelihood: 9668.697265625 \n",
      "Epoch:1-4/32, Negative loglikelihood: 9668.37109375 \n",
      "Epoch:1-5/32, Negative loglikelihood: 9667.7138671875 \n",
      "Epoch:1-6/32, Negative loglikelihood: 9666.830078125 \n",
      "Epoch:1-7/32, Negative loglikelihood: 9666.498046875 \n",
      "Epoch:1-8/32, Negative loglikelihood: 9667.69921875 \n",
      "Epoch:1-9/32, Negative loglikelihood: 9666.09765625 \n",
      "Epoch:1-10/32, Negative loglikelihood: 9354.8564453125 \n",
      "Epoch:1-11/32, Negative loglikelihood: 9665.0224609375 \n",
      "Epoch:1-12/32, Negative loglikelihood: 9664.22265625 \n",
      "Epoch:1-13/32, Negative loglikelihood: 9664.623046875 \n",
      "Epoch:1-14/32, Negative loglikelihood: 9663.5888671875 \n",
      "Epoch:1-15/32, Negative loglikelihood: 9039.2275390625 \n",
      "Epoch:1-16/32, Negative loglikelihood: 9350.6337890625 \n",
      "Epoch:1-17/32, Negative loglikelihood: 9662.189453125 \n",
      "Epoch:1-18/32, Negative loglikelihood: 9350.0537109375 \n",
      "Epoch:1-19/32, Negative loglikelihood: 9661.65234375 \n",
      "Epoch:1-20/32, Negative loglikelihood: 9661.8671875 \n",
      "Epoch:1-21/32, Negative loglikelihood: 9350.18359375 \n",
      "Epoch:1-22/32, Negative loglikelihood: 9350.1640625 \n",
      "Epoch:1-23/32, Negative loglikelihood: 9037.9853515625 \n",
      "Epoch:1-24/32, Negative loglikelihood: 9661.23046875 \n",
      "Epoch:1-25/32, Negative loglikelihood: 9661.0302734375 \n",
      "Epoch:1-26/32, Negative loglikelihood: 9661.3623046875 \n",
      "Epoch:1-27/32, Negative loglikelihood: 9660.396484375 \n",
      "Epoch:1-28/32, Negative loglikelihood: 9660.533203125 \n",
      "Epoch:1-29/32, Negative loglikelihood: 9660.498046875 \n",
      "Epoch:1-30/32, Negative loglikelihood: 9660.7607421875 \n",
      "Epoch:1-31/32, Negative loglikelihood: 7478.740234375 \n",
      "--------------------------------------------------------------\n",
      "Epoch:1 completed, Total training's Loss: 304267.2626953125, Spend: 0.06193496386210124m\n",
      "F1-Score: 0.9883507050889024\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       0.98      0.98      0.98       203\n",
      "        NAME       0.99      0.99      0.99       208\n",
      "         ORG       0.99      1.00      0.99       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:1, Acc:99.64, on Valid_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:2-0/32, Negative loglikelihood: 9660.083984375 \n",
      "Epoch:2-1/32, Negative loglikelihood: 9659.98828125 \n",
      "Epoch:2-2/32, Negative loglikelihood: 9348.2734375 \n",
      "Epoch:2-3/32, Negative loglikelihood: 9659.73046875 \n",
      "Epoch:2-4/32, Negative loglikelihood: 9659.890625 \n",
      "Epoch:2-5/32, Negative loglikelihood: 9659.642578125 \n",
      "Epoch:2-6/32, Negative loglikelihood: 9659.9619140625 \n",
      "Epoch:2-7/32, Negative loglikelihood: 9348.23046875 \n",
      "Epoch:2-8/32, Negative loglikelihood: 9659.5751953125 \n",
      "Epoch:2-9/32, Negative loglikelihood: 9659.537109375 \n",
      "Epoch:2-10/32, Negative loglikelihood: 9036.453125 \n",
      "Epoch:2-11/32, Negative loglikelihood: 9659.4423828125 \n",
      "Epoch:2-12/32, Negative loglikelihood: 9659.482421875 \n",
      "Epoch:2-13/32, Negative loglikelihood: 9659.390625 \n",
      "Epoch:2-14/32, Negative loglikelihood: 9659.11328125 \n",
      "Epoch:2-15/32, Negative loglikelihood: 9659.04296875 \n",
      "Epoch:2-16/32, Negative loglikelihood: 9347.4677734375 \n",
      "Epoch:2-17/32, Negative loglikelihood: 9658.884765625 \n",
      "Epoch:2-18/32, Negative loglikelihood: 9659.5576171875 \n",
      "Epoch:2-19/32, Negative loglikelihood: 9659.2177734375 \n",
      "Epoch:2-20/32, Negative loglikelihood: 9660.060546875 \n",
      "Epoch:2-21/32, Negative loglikelihood: 9658.8701171875 \n",
      "Epoch:2-22/32, Negative loglikelihood: 9347.2265625 \n",
      "Epoch:2-23/32, Negative loglikelihood: 9658.736328125 \n",
      "Epoch:2-24/32, Negative loglikelihood: 9659.087890625 \n",
      "Epoch:2-25/32, Negative loglikelihood: 9658.73828125 \n",
      "Epoch:2-26/32, Negative loglikelihood: 9347.53125 \n",
      "Epoch:2-27/32, Negative loglikelihood: 9347.130859375 \n",
      "Epoch:2-28/32, Negative loglikelihood: 9658.1640625 \n",
      "Epoch:2-29/32, Negative loglikelihood: 9346.658203125 \n",
      "Epoch:2-30/32, Negative loglikelihood: 9658.638671875 \n",
      "Epoch:2-31/32, Negative loglikelihood: 7477.26318359375 \n",
      "--------------------------------------------------------------\n",
      "Epoch:2 completed, Total training's Loss: 304111.07275390625, Spend: 0.07117368380228678m\n",
      "F1-Score: 0.9877450980392156\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       0.98      0.99      0.98       203\n",
      "        NAME       0.98      0.99      0.98       208\n",
      "         ORG       1.00      1.00      1.00       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:2, Acc:99.68, on Valid_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:3-0/32, Negative loglikelihood: 9658.275390625 \n",
      "Epoch:3-1/32, Negative loglikelihood: 9346.9267578125 \n",
      "Epoch:3-2/32, Negative loglikelihood: 9658.0703125 \n",
      "Epoch:3-3/32, Negative loglikelihood: 9346.427734375 \n",
      "Epoch:3-4/32, Negative loglikelihood: 9658.1943359375 \n",
      "Epoch:3-5/32, Negative loglikelihood: 9658.2490234375 \n",
      "Epoch:3-6/32, Negative loglikelihood: 9657.89453125 \n",
      "Epoch:3-7/32, Negative loglikelihood: 9657.767578125 \n",
      "Epoch:3-8/32, Negative loglikelihood: 9657.966796875 \n",
      "Epoch:3-9/32, Negative loglikelihood: 9346.46484375 \n",
      "Epoch:3-10/32, Negative loglikelihood: 9346.361328125 \n",
      "Epoch:3-11/32, Negative loglikelihood: 9657.890625 \n",
      "Epoch:3-12/32, Negative loglikelihood: 8724.4169921875 \n",
      "Epoch:3-13/32, Negative loglikelihood: 9657.5244140625 \n",
      "Epoch:3-14/32, Negative loglikelihood: 9657.818359375 \n",
      "Epoch:3-15/32, Negative loglikelihood: 9657.64453125 \n",
      "Epoch:3-16/32, Negative loglikelihood: 9657.763671875 \n",
      "Epoch:3-17/32, Negative loglikelihood: 9657.4296875 \n",
      "Epoch:3-18/32, Negative loglikelihood: 9346.2265625 \n",
      "Epoch:3-19/32, Negative loglikelihood: 9657.734375 \n",
      "Epoch:3-20/32, Negative loglikelihood: 9657.2890625 \n",
      "Epoch:3-21/32, Negative loglikelihood: 9657.462890625 \n",
      "Epoch:3-22/32, Negative loglikelihood: 9657.06640625 \n",
      "Epoch:3-23/32, Negative loglikelihood: 9034.296875 \n",
      "Epoch:3-24/32, Negative loglikelihood: 9657.5048828125 \n",
      "Epoch:3-25/32, Negative loglikelihood: 9657.5859375 \n",
      "Epoch:3-26/32, Negative loglikelihood: 9657.4697265625 \n",
      "Epoch:3-27/32, Negative loglikelihood: 9657.2412109375 \n",
      "Epoch:3-28/32, Negative loglikelihood: 9345.533203125 \n",
      "Epoch:3-29/32, Negative loglikelihood: 9656.900390625 \n",
      "Epoch:3-30/32, Negative loglikelihood: 9657.234375 \n",
      "Epoch:3-31/32, Negative loglikelihood: 8722.4248046875 \n",
      "--------------------------------------------------------------\n",
      "Epoch:3 completed, Total training's Loss: 304685.0576171875, Spend: 0.07112092177073161m\n",
      "F1-Score: 0.9920294297976702\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       0.99      0.99      0.99       203\n",
      "        NAME       0.99      1.00      0.99       208\n",
      "         ORG       1.00      1.00      1.00       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:3, Acc:99.76, on Valid_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:4-0/32, Negative loglikelihood: 9656.9931640625 \n",
      "Epoch:4-1/32, Negative loglikelihood: 9656.8076171875 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4-2/32, Negative loglikelihood: 9656.587890625 \n",
      "Epoch:4-3/32, Negative loglikelihood: 9345.1455078125 \n",
      "Epoch:4-4/32, Negative loglikelihood: 9656.73046875 \n",
      "Epoch:4-5/32, Negative loglikelihood: 9656.875 \n",
      "Epoch:4-6/32, Negative loglikelihood: 9656.365234375 \n",
      "Epoch:4-7/32, Negative loglikelihood: 9656.4931640625 \n",
      "Epoch:4-8/32, Negative loglikelihood: 9656.2041015625 \n",
      "Epoch:4-9/32, Negative loglikelihood: 9344.8623046875 \n",
      "Epoch:4-10/32, Negative loglikelihood: 9656.63671875 \n",
      "Epoch:4-11/32, Negative loglikelihood: 9656.4765625 \n",
      "Epoch:4-12/32, Negative loglikelihood: 9657.43359375 \n",
      "Epoch:4-13/32, Negative loglikelihood: 9656.671875 \n",
      "Epoch:4-14/32, Negative loglikelihood: 9656.4111328125 \n",
      "Epoch:4-15/32, Negative loglikelihood: 9344.6982421875 \n",
      "Epoch:4-16/32, Negative loglikelihood: 9344.794921875 \n",
      "Epoch:4-17/32, Negative loglikelihood: 9656.1865234375 \n",
      "Epoch:4-18/32, Negative loglikelihood: 9345.4853515625 \n",
      "Epoch:4-19/32, Negative loglikelihood: 9344.75 \n",
      "Epoch:4-20/32, Negative loglikelihood: 9656.076171875 \n",
      "Epoch:4-21/32, Negative loglikelihood: 9656.185546875 \n",
      "Epoch:4-22/32, Negative loglikelihood: 9656.1796875 \n",
      "Epoch:4-23/32, Negative loglikelihood: 9656.5078125 \n",
      "Epoch:4-24/32, Negative loglikelihood: 9656.240234375 \n",
      "Epoch:4-25/32, Negative loglikelihood: 9655.990234375 \n",
      "Epoch:4-26/32, Negative loglikelihood: 9655.94140625 \n",
      "Epoch:4-27/32, Negative loglikelihood: 9033.1162109375 \n",
      "Epoch:4-28/32, Negative loglikelihood: 9655.8984375 \n",
      "Epoch:4-29/32, Negative loglikelihood: 9655.701171875 \n",
      "Epoch:4-30/32, Negative loglikelihood: 9656.0830078125 \n",
      "Epoch:4-31/32, Negative loglikelihood: 6230.18212890625 \n",
      "--------------------------------------------------------------\n",
      "Epoch:4 completed, Total training's Loss: 303086.71142578125, Spend: 0.06283681392669678m\n",
      "F1-Score: 0.9901840490797547\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       0.99      0.99      0.99       203\n",
      "        NAME       0.99      0.99      0.99       208\n",
      "         ORG       1.00      1.00      1.00       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:4, Acc:99.72, on Valid_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:5-0/32, Negative loglikelihood: 9656.0029296875 \n",
      "Epoch:5-1/32, Negative loglikelihood: 9032.90625 \n",
      "Epoch:5-2/32, Negative loglikelihood: 9032.791015625 \n",
      "Epoch:5-3/32, Negative loglikelihood: 9655.884765625 \n",
      "Epoch:5-4/32, Negative loglikelihood: 9655.9072265625 \n",
      "Epoch:5-5/32, Negative loglikelihood: 9655.46484375 \n",
      "Epoch:5-6/32, Negative loglikelihood: 9344.0576171875 \n",
      "Epoch:5-7/32, Negative loglikelihood: 9655.18359375 \n",
      "Epoch:5-8/32, Negative loglikelihood: 9655.568359375 \n",
      "Epoch:5-9/32, Negative loglikelihood: 9656.6240234375 \n",
      "Epoch:5-10/32, Negative loglikelihood: 9655.5546875 \n",
      "Epoch:5-11/32, Negative loglikelihood: 9655.3828125 \n",
      "Epoch:5-12/32, Negative loglikelihood: 9655.41796875 \n",
      "Epoch:5-13/32, Negative loglikelihood: 9655.3603515625 \n",
      "Epoch:5-14/32, Negative loglikelihood: 9344.0546875 \n",
      "Epoch:5-15/32, Negative loglikelihood: 9032.748046875 \n",
      "Epoch:5-16/32, Negative loglikelihood: 9655.498046875 \n",
      "Epoch:5-17/32, Negative loglikelihood: 9655.38671875 \n",
      "Epoch:5-18/32, Negative loglikelihood: 9655.2412109375 \n",
      "Epoch:5-19/32, Negative loglikelihood: 9655.568359375 \n",
      "Epoch:5-20/32, Negative loglikelihood: 9343.9501953125 \n",
      "Epoch:5-21/32, Negative loglikelihood: 9654.951171875 \n",
      "Epoch:5-22/32, Negative loglikelihood: 9655.3203125 \n",
      "Epoch:5-23/32, Negative loglikelihood: 9655.41015625 \n",
      "Epoch:5-24/32, Negative loglikelihood: 9654.908203125 \n",
      "Epoch:5-25/32, Negative loglikelihood: 9654.79296875 \n",
      "Epoch:5-26/32, Negative loglikelihood: 9655.21875 \n",
      "Epoch:5-27/32, Negative loglikelihood: 9655.365234375 \n",
      "Epoch:5-28/32, Negative loglikelihood: 9654.896484375 \n",
      "Epoch:5-29/32, Negative loglikelihood: 9655.21484375 \n",
      "Epoch:5-30/32, Negative loglikelihood: 9654.7109375 \n",
      "Epoch:5-31/32, Negative loglikelihood: 8720.595703125 \n",
      "--------------------------------------------------------------\n",
      "Epoch:5 completed, Total training's Loss: 305235.9384765625, Spend: 0.062359170118967695m\n",
      "F1-Score: 0.992638036809816\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       0.99      0.99      0.99       203\n",
      "        NAME       0.99      1.00      0.99       208\n",
      "         ORG       1.00      1.00      1.00       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:5, Acc:99.72, on Valid_set, Spend:0.007 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:6-0/32, Negative loglikelihood: 9654.75 \n",
      "Epoch:6-1/32, Negative loglikelihood: 9654.83984375 \n",
      "Epoch:6-2/32, Negative loglikelihood: 9654.85546875 \n",
      "Epoch:6-3/32, Negative loglikelihood: 9654.908203125 \n",
      "Epoch:6-4/32, Negative loglikelihood: 9654.853515625 \n",
      "Epoch:6-5/32, Negative loglikelihood: 9655.5576171875 \n",
      "Epoch:6-6/32, Negative loglikelihood: 9654.474609375 \n",
      "Epoch:6-7/32, Negative loglikelihood: 9654.685546875 \n",
      "Epoch:6-8/32, Negative loglikelihood: 9654.39453125 \n",
      "Epoch:6-9/32, Negative loglikelihood: 9654.689453125 \n",
      "Epoch:6-10/32, Negative loglikelihood: 9654.671875 \n",
      "Epoch:6-11/32, Negative loglikelihood: 9654.5498046875 \n",
      "Epoch:6-12/32, Negative loglikelihood: 8720.447265625 \n",
      "Epoch:6-13/32, Negative loglikelihood: 9654.486328125 \n",
      "Epoch:6-14/32, Negative loglikelihood: 9654.341796875 \n",
      "Epoch:6-15/32, Negative loglikelihood: 9654.115234375 \n",
      "Epoch:6-16/32, Negative loglikelihood: 9654.2685546875 \n",
      "Epoch:6-17/32, Negative loglikelihood: 9654.515625 \n",
      "Epoch:6-18/32, Negative loglikelihood: 9654.484375 \n",
      "Epoch:6-19/32, Negative loglikelihood: 9654.2578125 \n",
      "Epoch:6-20/32, Negative loglikelihood: 9654.3740234375 \n",
      "Epoch:6-21/32, Negative loglikelihood: 9342.90234375 \n",
      "Epoch:6-22/32, Negative loglikelihood: 9654.4814453125 \n",
      "Epoch:6-23/32, Negative loglikelihood: 9654.236328125 \n",
      "Epoch:6-24/32, Negative loglikelihood: 9654.017578125 \n",
      "Epoch:6-25/32, Negative loglikelihood: 9654.2568359375 \n",
      "Epoch:6-26/32, Negative loglikelihood: 9030.9775390625 \n",
      "Epoch:6-27/32, Negative loglikelihood: 9653.9306640625 \n",
      "Epoch:6-28/32, Negative loglikelihood: 9342.439453125 \n",
      "Epoch:6-29/32, Negative loglikelihood: 9653.896484375 \n",
      "Epoch:6-30/32, Negative loglikelihood: 9654.2119140625 \n",
      "Epoch:6-31/32, Negative loglikelihood: 8719.779296875 \n",
      "--------------------------------------------------------------\n",
      "Epoch:6 completed, Total training's Loss: 305827.6513671875, Spend: 0.06791148980458578m\n",
      "F1-Score: 0.9920294297976702\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       0.99      0.99      0.99       203\n",
      "        NAME       0.99      1.00      0.99       208\n",
      "         ORG       1.00      1.00      1.00       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:6, Acc:99.76, on Valid_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:7-0/32, Negative loglikelihood: 9654.025390625 \n",
      "Epoch:7-1/32, Negative loglikelihood: 9653.7041015625 \n",
      "Epoch:7-2/32, Negative loglikelihood: 9342.3388671875 \n",
      "Epoch:7-3/32, Negative loglikelihood: 9654.017578125 \n",
      "Epoch:7-4/32, Negative loglikelihood: 9653.9580078125 \n",
      "Epoch:7-5/32, Negative loglikelihood: 9653.982421875 \n",
      "Epoch:7-6/32, Negative loglikelihood: 9653.896484375 \n",
      "Epoch:7-7/32, Negative loglikelihood: 8719.66015625 \n",
      "Epoch:7-8/32, Negative loglikelihood: 9653.77734375 \n",
      "Epoch:7-9/32, Negative loglikelihood: 9653.685546875 \n",
      "Epoch:7-10/32, Negative loglikelihood: 9653.63671875 \n",
      "Epoch:7-11/32, Negative loglikelihood: 9653.33984375 \n",
      "Epoch:7-12/32, Negative loglikelihood: 9653.35546875 \n",
      "Epoch:7-13/32, Negative loglikelihood: 9653.458984375 \n",
      "Epoch:7-14/32, Negative loglikelihood: 9653.591796875 \n",
      "Epoch:7-15/32, Negative loglikelihood: 9653.4873046875 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7-16/32, Negative loglikelihood: 9030.7314453125 \n",
      "Epoch:7-17/32, Negative loglikelihood: 9653.564453125 \n",
      "Epoch:7-18/32, Negative loglikelihood: 9653.28515625 \n",
      "Epoch:7-19/32, Negative loglikelihood: 9653.4423828125 \n",
      "Epoch:7-20/32, Negative loglikelihood: 9654.3466796875 \n",
      "Epoch:7-21/32, Negative loglikelihood: 9653.625 \n",
      "Epoch:7-22/32, Negative loglikelihood: 9653.310546875 \n",
      "Epoch:7-23/32, Negative loglikelihood: 9653.431640625 \n",
      "Epoch:7-24/32, Negative loglikelihood: 9653.2294921875 \n",
      "Epoch:7-25/32, Negative loglikelihood: 9342.37109375 \n",
      "Epoch:7-26/32, Negative loglikelihood: 9653.5400390625 \n",
      "Epoch:7-27/32, Negative loglikelihood: 9653.29296875 \n",
      "Epoch:7-28/32, Negative loglikelihood: 9652.9736328125 \n",
      "Epoch:7-29/32, Negative loglikelihood: 9652.9609375 \n",
      "Epoch:7-30/32, Negative loglikelihood: 9653.2333984375 \n",
      "Epoch:7-31/32, Negative loglikelihood: 7473.2333984375 \n",
      "--------------------------------------------------------------\n",
      "Epoch:7 completed, Total training's Loss: 304554.48828125, Spend: 0.06996166308720907m\n",
      "F1-Score: 0.9920294297976702\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       0.99      0.99      0.99       203\n",
      "        NAME       0.99      1.00      0.99       208\n",
      "         ORG       1.00      1.00      1.00       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:7, Acc:99.76, on Valid_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:8-0/32, Negative loglikelihood: 9341.7734375 \n",
      "Epoch:8-1/32, Negative loglikelihood: 9653.064453125 \n",
      "Epoch:8-2/32, Negative loglikelihood: 9653.34375 \n",
      "Epoch:8-3/32, Negative loglikelihood: 9653.439453125 \n",
      "Epoch:8-4/32, Negative loglikelihood: 9653.318359375 \n",
      "Epoch:8-5/32, Negative loglikelihood: 9652.775390625 \n",
      "Epoch:8-6/32, Negative loglikelihood: 9653.052734375 \n",
      "Epoch:8-7/32, Negative loglikelihood: 9652.927734375 \n",
      "Epoch:8-8/32, Negative loglikelihood: 9653.32421875 \n",
      "Epoch:8-9/32, Negative loglikelihood: 9341.9033203125 \n",
      "Epoch:8-10/32, Negative loglikelihood: 9652.900390625 \n",
      "Epoch:8-11/32, Negative loglikelihood: 9652.978515625 \n",
      "Epoch:8-12/32, Negative loglikelihood: 9341.3017578125 \n",
      "Epoch:8-13/32, Negative loglikelihood: 9652.9775390625 \n",
      "Epoch:8-14/32, Negative loglikelihood: 9652.7568359375 \n",
      "Epoch:8-15/32, Negative loglikelihood: 9030.44921875 \n",
      "Epoch:8-16/32, Negative loglikelihood: 9029.90234375 \n",
      "Epoch:8-17/32, Negative loglikelihood: 9652.791015625 \n",
      "Epoch:8-18/32, Negative loglikelihood: 9652.9267578125 \n",
      "Epoch:8-19/32, Negative loglikelihood: 9652.865234375 \n",
      "Epoch:8-20/32, Negative loglikelihood: 9652.6015625 \n",
      "Epoch:8-21/32, Negative loglikelihood: 9652.634765625 \n",
      "Epoch:8-22/32, Negative loglikelihood: 9652.91796875 \n",
      "Epoch:8-23/32, Negative loglikelihood: 9341.107421875 \n",
      "Epoch:8-24/32, Negative loglikelihood: 9652.7421875 \n",
      "Epoch:8-25/32, Negative loglikelihood: 9652.6572265625 \n",
      "Epoch:8-26/32, Negative loglikelihood: 9653.0673828125 \n",
      "Epoch:8-27/32, Negative loglikelihood: 9652.318359375 \n",
      "Epoch:8-28/32, Negative loglikelihood: 9652.5859375 \n",
      "Epoch:8-29/32, Negative loglikelihood: 9652.23828125 \n",
      "Epoch:8-30/32, Negative loglikelihood: 9652.392578125 \n",
      "Epoch:8-31/32, Negative loglikelihood: 8718.375 \n",
      "--------------------------------------------------------------\n",
      "Epoch:8 completed, Total training's Loss: 305466.4111328125, Spend: 0.06292266050974528m\n",
      "F1-Score: 0.992638036809816\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       0.99      0.99      0.99       203\n",
      "        NAME       0.99      1.00      0.99       208\n",
      "         ORG       1.00      1.00      1.00       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:8, Acc:99.72, on Valid_set, Spend:0.007 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:9-0/32, Negative loglikelihood: 9652.5732421875 \n",
      "Epoch:9-1/32, Negative loglikelihood: 9652.57421875 \n",
      "Epoch:9-2/32, Negative loglikelihood: 9652.638671875 \n",
      "Epoch:9-3/32, Negative loglikelihood: 9652.7216796875 \n",
      "Epoch:9-4/32, Negative loglikelihood: 9652.3896484375 \n",
      "Epoch:9-5/32, Negative loglikelihood: 9652.375 \n",
      "Epoch:9-6/32, Negative loglikelihood: 9652.451171875 \n",
      "Epoch:9-7/32, Negative loglikelihood: 9341.0546875 \n",
      "Epoch:9-8/32, Negative loglikelihood: 9652.189453125 \n",
      "Epoch:9-9/32, Negative loglikelihood: 9340.796875 \n",
      "Epoch:9-10/32, Negative loglikelihood: 9340.630859375 \n",
      "Epoch:9-11/32, Negative loglikelihood: 9651.9951171875 \n",
      "Epoch:9-12/32, Negative loglikelihood: 9652.458984375 \n",
      "Epoch:9-13/32, Negative loglikelihood: 9652.365234375 \n",
      "Epoch:9-14/32, Negative loglikelihood: 9652.037109375 \n",
      "Epoch:9-15/32, Negative loglikelihood: 9652.123046875 \n",
      "Epoch:9-16/32, Negative loglikelihood: 9651.896484375 \n",
      "Epoch:9-17/32, Negative loglikelihood: 9340.66796875 \n",
      "Epoch:9-18/32, Negative loglikelihood: 9652.162109375 \n",
      "Epoch:9-19/32, Negative loglikelihood: 9652.2333984375 \n",
      "Epoch:9-20/32, Negative loglikelihood: 9652.271484375 \n",
      "Epoch:9-21/32, Negative loglikelihood: 9652.365234375 \n",
      "Epoch:9-22/32, Negative loglikelihood: 9652.0283203125 \n",
      "Epoch:9-23/32, Negative loglikelihood: 9651.5888671875 \n",
      "Epoch:9-24/32, Negative loglikelihood: 9340.9482421875 \n",
      "Epoch:9-25/32, Negative loglikelihood: 9651.865234375 \n",
      "Epoch:9-26/32, Negative loglikelihood: 8095.4912109375 \n",
      "Epoch:9-27/32, Negative loglikelihood: 9651.419921875 \n",
      "Epoch:9-28/32, Negative loglikelihood: 9652.322265625 \n",
      "Epoch:9-29/32, Negative loglikelihood: 9651.869140625 \n",
      "Epoch:9-30/32, Negative loglikelihood: 9029.37890625 \n",
      "Epoch:9-31/32, Negative loglikelihood: 7472.01123046875 \n",
      "--------------------------------------------------------------\n",
      "Epoch:9 completed, Total training's Loss: 302953.89501953125, Spend: 0.06265283425649007m\n",
      "F1-Score: 0.992638036809816\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       0.99      0.99      0.99       203\n",
      "        NAME       0.99      1.00      0.99       208\n",
      "         ORG       1.00      1.00      1.00       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:9, Acc:99.72, on Valid_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:10-0/32, Negative loglikelihood: 9651.921875 \n",
      "Epoch:10-1/32, Negative loglikelihood: 9651.642578125 \n",
      "Epoch:10-2/32, Negative loglikelihood: 9340.6279296875 \n",
      "Epoch:10-3/32, Negative loglikelihood: 9651.859375 \n",
      "Epoch:10-4/32, Negative loglikelihood: 9652.052734375 \n",
      "Epoch:10-5/32, Negative loglikelihood: 9651.73046875 \n",
      "Epoch:10-6/32, Negative loglikelihood: 9652.21484375 \n",
      "Epoch:10-7/32, Negative loglikelihood: 9651.65234375 \n",
      "Epoch:10-8/32, Negative loglikelihood: 9652.029296875 \n",
      "Epoch:10-9/32, Negative loglikelihood: 9651.7509765625 \n",
      "Epoch:10-10/32, Negative loglikelihood: 9651.3330078125 \n",
      "Epoch:10-11/32, Negative loglikelihood: 9651.466796875 \n",
      "Epoch:10-12/32, Negative loglikelihood: 9651.748046875 \n",
      "Epoch:10-13/32, Negative loglikelihood: 9651.232421875 \n",
      "Epoch:10-14/32, Negative loglikelihood: 9651.4501953125 \n",
      "Epoch:10-15/32, Negative loglikelihood: 9650.98828125 \n",
      "Epoch:10-16/32, Negative loglikelihood: 9029.103515625 \n",
      "Epoch:10-17/32, Negative loglikelihood: 9651.5517578125 \n",
      "Epoch:10-18/32, Negative loglikelihood: 9651.724609375 \n",
      "Epoch:10-19/32, Negative loglikelihood: 9651.6484375 \n",
      "Epoch:10-20/32, Negative loglikelihood: 8717.5849609375 \n",
      "Epoch:10-21/32, Negative loglikelihood: 9651.95703125 \n",
      "Epoch:10-22/32, Negative loglikelihood: 9651.556640625 \n",
      "Epoch:10-23/32, Negative loglikelihood: 9651.5732421875 \n",
      "Epoch:10-24/32, Negative loglikelihood: 9340.41796875 \n",
      "Epoch:10-25/32, Negative loglikelihood: 9340.3330078125 \n",
      "Epoch:10-26/32, Negative loglikelihood: 9651.818359375 \n",
      "Epoch:10-27/32, Negative loglikelihood: 9651.8359375 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10-28/32, Negative loglikelihood: 9651.275390625 \n",
      "Epoch:10-29/32, Negative loglikelihood: 9651.6337890625 \n",
      "Epoch:10-30/32, Negative loglikelihood: 9651.2509765625 \n",
      "Epoch:10-31/32, Negative loglikelihood: 8717.4765625 \n",
      "--------------------------------------------------------------\n",
      "Epoch:10 completed, Total training's Loss: 305428.443359375, Spend: 0.0631765325864156m\n",
      "F1-Score: 0.9932473910374464\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       1.00      0.99      0.99       203\n",
      "        NAME       0.99      1.00      0.99       208\n",
      "         ORG       1.00      1.00      1.00       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:10, Acc:99.68, on Valid_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:11-0/32, Negative loglikelihood: 9651.56640625 \n",
      "Epoch:11-1/32, Negative loglikelihood: 9651.87890625 \n",
      "Epoch:11-2/32, Negative loglikelihood: 9651.265625 \n",
      "Epoch:11-3/32, Negative loglikelihood: 9340.2529296875 \n",
      "Epoch:11-4/32, Negative loglikelihood: 9651.361328125 \n",
      "Epoch:11-5/32, Negative loglikelihood: 9651.255859375 \n",
      "Epoch:11-6/32, Negative loglikelihood: 9651.8203125 \n",
      "Epoch:11-7/32, Negative loglikelihood: 9340.1201171875 \n",
      "Epoch:11-8/32, Negative loglikelihood: 9650.935546875 \n",
      "Epoch:11-9/32, Negative loglikelihood: 9651.30859375 \n",
      "Epoch:11-10/32, Negative loglikelihood: 9651.49609375 \n",
      "Epoch:11-11/32, Negative loglikelihood: 9651.1611328125 \n",
      "Epoch:11-12/32, Negative loglikelihood: 9651.244140625 \n",
      "Epoch:11-13/32, Negative loglikelihood: 9651.0810546875 \n",
      "Epoch:11-14/32, Negative loglikelihood: 9651.1953125 \n",
      "Epoch:11-15/32, Negative loglikelihood: 9651.1630859375 \n",
      "Epoch:11-16/32, Negative loglikelihood: 9340.099609375 \n",
      "Epoch:11-17/32, Negative loglikelihood: 9651.2197265625 \n",
      "Epoch:11-18/32, Negative loglikelihood: 8717.4755859375 \n",
      "Epoch:11-19/32, Negative loglikelihood: 9651.58984375 \n",
      "Epoch:11-20/32, Negative loglikelihood: 9651.0927734375 \n",
      "Epoch:11-21/32, Negative loglikelihood: 9651.30078125 \n",
      "Epoch:11-22/32, Negative loglikelihood: 9651.056640625 \n",
      "Epoch:11-23/32, Negative loglikelihood: 9651.322265625 \n",
      "Epoch:11-24/32, Negative loglikelihood: 9651.294921875 \n",
      "Epoch:11-25/32, Negative loglikelihood: 9651.228515625 \n",
      "Epoch:11-26/32, Negative loglikelihood: 9650.861328125 \n",
      "Epoch:11-27/32, Negative loglikelihood: 9651.3046875 \n",
      "Epoch:11-28/32, Negative loglikelihood: 9650.97265625 \n",
      "Epoch:11-29/32, Negative loglikelihood: 9651.3427734375 \n",
      "Epoch:11-30/32, Negative loglikelihood: 9340.0947265625 \n",
      "Epoch:11-31/32, Negative loglikelihood: 8717.04296875 \n",
      "--------------------------------------------------------------\n",
      "Epoch:11 completed, Total training's Loss: 305728.40625, Spend: 0.07613914012908936m\n",
      "F1-Score: 0.992638036809816\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       0.99      0.99      0.99       203\n",
      "        NAME       0.99      1.00      0.99       208\n",
      "         ORG       1.00      1.00      1.00       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:11, Acc:99.72, on Valid_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:12-0/32, Negative loglikelihood: 9339.7275390625 \n",
      "Epoch:12-1/32, Negative loglikelihood: 9650.8515625 \n",
      "Epoch:12-2/32, Negative loglikelihood: 9651.0439453125 \n",
      "Epoch:12-3/32, Negative loglikelihood: 8717.119140625 \n",
      "Epoch:12-4/32, Negative loglikelihood: 9650.970703125 \n",
      "Epoch:12-5/32, Negative loglikelihood: 9650.8388671875 \n",
      "Epoch:12-6/32, Negative loglikelihood: 9651.4677734375 \n",
      "Epoch:12-7/32, Negative loglikelihood: 9650.798828125 \n",
      "Epoch:12-8/32, Negative loglikelihood: 9651.103515625 \n",
      "Epoch:12-9/32, Negative loglikelihood: 9651.01953125 \n",
      "Epoch:12-10/32, Negative loglikelihood: 9650.701171875 \n",
      "Epoch:12-11/32, Negative loglikelihood: 9651.228515625 \n",
      "Epoch:12-12/32, Negative loglikelihood: 9339.7421875 \n",
      "Epoch:12-13/32, Negative loglikelihood: 9650.77734375 \n",
      "Epoch:12-14/32, Negative loglikelihood: 9651.1015625 \n",
      "Epoch:12-15/32, Negative loglikelihood: 9339.9140625 \n",
      "Epoch:12-16/32, Negative loglikelihood: 9339.6826171875 \n",
      "Epoch:12-17/32, Negative loglikelihood: 9028.587890625 \n",
      "Epoch:12-18/32, Negative loglikelihood: 9339.626953125 \n",
      "Epoch:12-19/32, Negative loglikelihood: 9650.8408203125 \n",
      "Epoch:12-20/32, Negative loglikelihood: 9651.076171875 \n",
      "Epoch:12-21/32, Negative loglikelihood: 9650.84375 \n",
      "Epoch:12-22/32, Negative loglikelihood: 9650.94921875 \n",
      "Epoch:12-23/32, Negative loglikelihood: 9339.453125 \n",
      "Epoch:12-24/32, Negative loglikelihood: 9339.455078125 \n",
      "Epoch:12-25/32, Negative loglikelihood: 9651.2001953125 \n",
      "Epoch:12-26/32, Negative loglikelihood: 9028.2744140625 \n",
      "Epoch:12-27/32, Negative loglikelihood: 9650.8671875 \n",
      "Epoch:12-28/32, Negative loglikelihood: 9650.72265625 \n",
      "Epoch:12-29/32, Negative loglikelihood: 9339.8828125 \n",
      "Epoch:12-30/32, Negative loglikelihood: 9650.90625 \n",
      "Epoch:12-31/32, Negative loglikelihood: 8716.4609375 \n",
      "--------------------------------------------------------------\n",
      "Epoch:12 completed, Total training's Loss: 303227.236328125, Spend: 0.07121730248133341m\n",
      "F1-Score: 0.992638036809816\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       0.99      0.99      0.99       203\n",
      "        NAME       0.99      1.00      0.99       208\n",
      "         ORG       1.00      1.00      1.00       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:12, Acc:99.72, on Valid_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Epoch:13-0/32, Negative loglikelihood: 9650.8759765625 \n",
      "Epoch:13-1/32, Negative loglikelihood: 9650.84375 \n",
      "Epoch:13-2/32, Negative loglikelihood: 9650.671875 \n",
      "Epoch:13-3/32, Negative loglikelihood: 9650.990234375 \n",
      "Epoch:13-4/32, Negative loglikelihood: 9339.556640625 \n",
      "Epoch:13-5/32, Negative loglikelihood: 9650.99609375 \n",
      "Epoch:13-6/32, Negative loglikelihood: 9650.84375 \n",
      "Epoch:13-7/32, Negative loglikelihood: 9339.49609375 \n",
      "Epoch:13-8/32, Negative loglikelihood: 9650.77734375 \n",
      "Epoch:13-9/32, Negative loglikelihood: 9650.64453125 \n",
      "Epoch:13-10/32, Negative loglikelihood: 9650.4365234375 \n",
      "Epoch:13-11/32, Negative loglikelihood: 9650.677734375 \n",
      "Epoch:13-12/32, Negative loglikelihood: 9650.6728515625 \n",
      "Epoch:13-13/32, Negative loglikelihood: 9651.0234375 \n",
      "Epoch:13-14/32, Negative loglikelihood: 9650.84375 \n",
      "Epoch:13-15/32, Negative loglikelihood: 9650.7265625 \n",
      "Epoch:13-16/32, Negative loglikelihood: 9650.9140625 \n",
      "Epoch:13-17/32, Negative loglikelihood: 9650.73046875 \n",
      "Epoch:13-18/32, Negative loglikelihood: 9650.939453125 \n",
      "Epoch:13-19/32, Negative loglikelihood: 9650.94140625 \n",
      "Epoch:13-20/32, Negative loglikelihood: 8405.361328125 \n",
      "Epoch:13-21/32, Negative loglikelihood: 9339.638671875 \n",
      "Epoch:13-22/32, Negative loglikelihood: 9028.2421875 \n",
      "Epoch:13-23/32, Negative loglikelihood: 9028.103515625 \n",
      "Epoch:13-24/32, Negative loglikelihood: 9650.904296875 \n",
      "Epoch:13-25/32, Negative loglikelihood: 9650.845703125 \n",
      "Epoch:13-26/32, Negative loglikelihood: 9650.595703125 \n",
      "Epoch:13-27/32, Negative loglikelihood: 9650.78125 \n",
      "Epoch:13-28/32, Negative loglikelihood: 9651.10546875 \n",
      "Epoch:13-29/32, Negative loglikelihood: 9028.02734375 \n",
      "Epoch:13-30/32, Negative loglikelihood: 9650.5927734375 \n",
      "Epoch:13-31/32, Negative loglikelihood: 8716.9091796875 \n",
      "--------------------------------------------------------------\n",
      "Epoch:13 completed, Total training's Loss: 303844.7099609375, Spend: 0.07083272139231364m\n",
      "F1-Score: 0.9920294297976702\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       0.99      0.99      0.99       203\n",
      "        NAME       0.99      1.00      0.99       208\n",
      "         ORG       1.00      1.00      1.00       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:13, Acc:99.76, on Valid_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:14-0/32, Negative loglikelihood: 9339.796875 \n",
      "Epoch:14-1/32, Negative loglikelihood: 9650.81640625 \n",
      "Epoch:14-2/32, Negative loglikelihood: 9650.6171875 \n",
      "Epoch:14-3/32, Negative loglikelihood: 9650.94921875 \n",
      "Epoch:14-4/32, Negative loglikelihood: 9651.0615234375 \n",
      "Epoch:14-5/32, Negative loglikelihood: 9650.91796875 \n",
      "Epoch:14-6/32, Negative loglikelihood: 9650.80078125 \n",
      "Epoch:14-7/32, Negative loglikelihood: 9650.455078125 \n",
      "Epoch:14-8/32, Negative loglikelihood: 8716.8134765625 \n",
      "Epoch:14-9/32, Negative loglikelihood: 9650.9462890625 \n",
      "Epoch:14-10/32, Negative loglikelihood: 9650.3876953125 \n",
      "Epoch:14-11/32, Negative loglikelihood: 9650.380859375 \n",
      "Epoch:14-12/32, Negative loglikelihood: 9650.46484375 \n",
      "Epoch:14-13/32, Negative loglikelihood: 9651.048828125 \n",
      "Epoch:14-14/32, Negative loglikelihood: 9650.591796875 \n",
      "Epoch:14-15/32, Negative loglikelihood: 9650.833984375 \n",
      "Epoch:14-16/32, Negative loglikelihood: 9650.6962890625 \n",
      "Epoch:14-17/32, Negative loglikelihood: 9650.8896484375 \n",
      "Epoch:14-18/32, Negative loglikelihood: 9650.994140625 \n",
      "Epoch:14-19/32, Negative loglikelihood: 9650.8583984375 \n",
      "Epoch:14-20/32, Negative loglikelihood: 9339.599609375 \n",
      "Epoch:14-21/32, Negative loglikelihood: 9339.3125 \n",
      "Epoch:14-22/32, Negative loglikelihood: 9650.41796875 \n",
      "Epoch:14-23/32, Negative loglikelihood: 9650.4697265625 \n",
      "Epoch:14-24/32, Negative loglikelihood: 9650.837890625 \n",
      "Epoch:14-25/32, Negative loglikelihood: 9650.654296875 \n",
      "Epoch:14-26/32, Negative loglikelihood: 9650.560546875 \n",
      "Epoch:14-27/32, Negative loglikelihood: 9650.87890625 \n",
      "Epoch:14-28/32, Negative loglikelihood: 9650.859375 \n",
      "Epoch:14-29/32, Negative loglikelihood: 9650.65625 \n",
      "Epoch:14-30/32, Negative loglikelihood: 9650.732421875 \n",
      "Epoch:14-31/32, Negative loglikelihood: 7472.1494140625 \n",
      "--------------------------------------------------------------\n",
      "Epoch:14 completed, Total training's Loss: 304777.4501953125, Spend: 0.06395090023676554m\n",
      "F1-Score: 0.9920294297976702\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.99      0.99      0.99       200\n",
      "         LOC       0.99      0.99      0.99       203\n",
      "        NAME       0.99      1.00      0.99       208\n",
      "         ORG       1.00      1.00      1.00       203\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       814\n",
      "   macro avg       0.99      0.99      0.99       814\n",
      "weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Epoch:14, Acc:99.76, on Valid_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "F1-Score: 0.9913151364764269\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.98      0.98      0.98       200\n",
      "         LOC       0.99      0.99      0.99       199\n",
      "        NAME       1.00      1.00      1.00       204\n",
      "         ORG       0.99      1.00      0.99       201\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       804\n",
      "   macro avg       0.99      0.99      0.99       804\n",
      "weighted avg       0.99      0.99      0.99       804\n",
      "\n",
      "Epoch:14, Acc:99.71, on Test_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "Loaded the pretrain  NER_BERT_CRF  model, epoch: 10 valid acc: 0.9967663702506063 valid f1: 0.9932473910374464\n",
      "F1-Score: 0.9894606323620583\n",
      "Classification report: -- \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INDU       0.98      0.98      0.98       200\n",
      "         LOC       1.00      0.99      1.00       199\n",
      "        NAME       1.00      1.00      1.00       204\n",
      "         ORG       0.98      0.99      0.98       201\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       804\n",
      "   macro avg       0.99      0.99      0.99       804\n",
      "weighted avg       0.99      0.99      0.99       804\n",
      "\n",
      "Epoch:10, Acc:99.63, on Test_set, Spend:0.008 minutes for evaluation\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9962962962962963, 0.9894606323620583)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "# train procedure\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "global_step_th = int(len(train_examples) / batch_size / gradient_accumulation_steps * start_epoch)\n",
    "\n",
    "# train_start=time.time()\n",
    "for epoch in range(start_epoch, total_train_epochs):\n",
    "    tr_loss = 0\n",
    "    train_start = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "\n",
    "        neg_log_likelihood = model.neg_log_likelihood(input_ids, segment_ids, input_mask, label_ids)\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            neg_log_likelihood = neg_log_likelihood / gradient_accumulation_steps\n",
    "\n",
    "        neg_log_likelihood.backward()\n",
    "\n",
    "        tr_loss += neg_log_likelihood.item()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # modify learning rate with special warm up BERT uses\n",
    "            lr_this_step = learning_rate0 * warmup_linear(global_step_th/total_train_steps, warmup_proportion)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step_th += 1\n",
    "            \n",
    "        print(\"Epoch:{}-{}/{}, Negative loglikelihood: {} \".format(epoch, step, len(train_dataloader), neg_log_likelihood.item()))\n",
    "    \n",
    "    print('--------------------------------------------------------------')\n",
    "    print(\"Epoch:{} completed, Total training's Loss: {}, Spend: {}m\".format(epoch, tr_loss, (time.time() - train_start)/60.0))\n",
    "    \n",
    "    ### 当前 epoch 的 evaluate\n",
    "    valid_acc, valid_f1 = evaluate(model, dev_dataloader, batch_size, epoch, 'Valid_set')\n",
    "\n",
    "    # Save a checkpoint\n",
    "    if valid_f1 > valid_f1_prev:\n",
    "        torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'valid_acc': valid_acc,\n",
    "            'valid_f1': valid_f1, 'max_seq_length': max_seq_length, 'lower_case': do_lower_case},\n",
    "                    os.path.join(output_dir, 'ner_bert_crf_checkpoint.pt'))\n",
    "        valid_f1_prev = valid_f1\n",
    "\n",
    "\n",
    "\n",
    "evaluate(model, test_dataloader, batch_size, total_train_epochs-1, 'Test_set')\n",
    "\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "'''\n",
    "Test_set prediction using the best epoch of NER_BERT_CRF model\n",
    "'''\n",
    "checkpoint = torch.load(output_dir+'/ner_bert_crf_checkpoint.pt', map_location='cpu')\n",
    "epoch = checkpoint['epoch']\n",
    "valid_acc_prev = checkpoint['valid_acc']\n",
    "valid_f1_prev = checkpoint['valid_f1']\n",
    "pretrained_dict=checkpoint['model_state']\n",
    "net_state_dict = model.state_dict()\n",
    "pretrained_dict_selected = {k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "net_state_dict.update(pretrained_dict_selected)\n",
    "model.load_state_dict(net_state_dict)\n",
    "print('Loaded the pretrain  NER_BERT_CRF  model, epoch:',checkpoint['epoch'],'valid acc:', \n",
    "      checkpoint['valid_acc'], 'valid f1:', checkpoint['valid_f1'])\n",
    "\n",
    "model.to(device)\n",
    "#evaluate(model, train_dataloader, batch_size, total_train_epochs-1, 'Train_set')\n",
    "evaluate(model, test_dataloader, batch_size, epoch, 'Test_set')\n",
    "# print('Total spend:',(time.time()-train_start)/60.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wiser)",
   "language": "python",
   "name": "wiser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
